{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537440ee",
   "metadata": {},
   "source": [
    "\n",
    "# Clasificación de Razas de Mascotas — Oxford‑IIIT Pet (Keras/TensorFlow)\n",
    "\n",
    "**Luis Ubaldo Balderas Sanchez A01751150** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4af8a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introducción\n",
    "\n",
    "**Problema:** Clasificar imágenes de perros y gatos en **37 razas** usando **deep learning**.  \n",
    "**Relevancia:** Aplicaciones en catalogación automática, búsquedas por imagen y apoyo a refugios/ONGs para identificación.\n",
    "\n",
    "**Objetivo:** Entrenar un modelo CNN (transfer learning) y **mejorarlo** mediante fine‑tuning/regularización, evaluando con métricas de validación y prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446acd9c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Datos\n",
    "\n",
    "**Fuente:** Oxford‑IIIT Pet (via `tensorflow_datasets`).  \n",
    "- 37 clases de perros y gatos.  \n",
    "- En este notebook descargamos los datos automáticamente \n",
    "\n",
    "**Split:** Usaremos `train/val/test` (10% de train pasa a validación).  \n",
    "**Preprocesamiento:** Redimensionado a 224×224, normalización según la *preprocess* de la arquitectura base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b13754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, json, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from keras import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51215986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1 3.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, json, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from keras import layers, callbacks\n",
    "\n",
    "# Config reproducibilidad (opcional)\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = \"data/processed/pets\"\n",
    "IMG_SIZE = (224, 224)\n",
    "VAL_FRACTION = 0.10\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "print(tf.__version__, keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1ad2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Descarga/lectura del dataset con TFDS y exportación a carpetas\n",
    "def _save_example(img, label, class_names, split_dir, idx):\n",
    "    cls = class_names[int(label)]\n",
    "    class_dir = os.path.join(split_dir, cls)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(tf.clip_by_value(img, 0, 255), tf.uint8)\n",
    "    tf.io.write_file(os.path.join(class_dir, f\"{cls}_{idx:06d}.jpg\"), tf.io.encode_jpeg(img))\n",
    "\n",
    "def prepare_oxford_pets():\n",
    "    ds_train = tfds.load(\"oxford_iiit_pet\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
    "    ds_test  = tfds.load(\"oxford_iiit_pet\", split=\"test\",  as_supervised=True, shuffle_files=False)\n",
    "    info = tfds.builder(\"oxford_iiit_pet\").info\n",
    "    class_names = info.features[\"label\"].names\n",
    "    \n",
    "    # materializamos para split de validación reproducible\n",
    "    train_list = list(tfds.as_numpy(ds_train))\n",
    "    n_val = math.ceil(len(train_list) * VAL_FRACTION)\n",
    "    val_list = train_list[:n_val]\n",
    "    trn_list = train_list[n_val:]\n",
    "    \n",
    "    # crear carpetas\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for c in class_names:\n",
    "            os.makedirs(os.path.join(DATA_ROOT, split, c), exist_ok=True)\n",
    "    \n",
    "    i = 0\n",
    "    for img, label in trn_list:\n",
    "        _save_example(img, label, class_names, os.path.join(DATA_ROOT, \"train\"), i); i += 1\n",
    "    j = 0\n",
    "    for img, label in val_list:\n",
    "        _save_example(img, label, class_names, os.path.join(DATA_ROOT, \"val\"), j); j += 1\n",
    "    k = 0\n",
    "    for img, label in tfds.as_numpy(ds_test):\n",
    "        _save_example(img, label, class_names, os.path.join(DATA_ROOT, \"test\"), k); k += 1\n",
    "    \n",
    "    return class_names\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, \"train\")):\n",
    "    class_names = prepare_oxford_pets()\n",
    "else:\n",
    "    # leer nombres de clase desde carpetas\n",
    "    class_names = sorted([d for d in os.listdir(os.path.join(DATA_ROOT, \"train\")) if os.path.isdir(os.path.join(DATA_ROOT, \"train\", d))])\n",
    "\n",
    "len(class_names), class_names[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f429ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birman             95\n",
       "wheaten_terrier    95\n",
       "chihuahua          95\n",
       "samoyed            94\n",
       "keeshond           94\n",
       "leonberger         94\n",
       "Persian            94\n",
       "Sphynx             94\n",
       "pomeranian         93\n",
       "great_pyrenees     93\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Visualización rápida: distribución de clases (train)\n",
    "def class_distribution(split=\"train\"):\n",
    "    root = os.path.join(DATA_ROOT, split)\n",
    "    counts = {}\n",
    "    for c in class_names:\n",
    "        cdir = os.path.join(root, c)\n",
    "        counts[c] = len([f for f in os.listdir(cdir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
    "    return pd.Series(counts).sort_values(ascending=False)\n",
    "\n",
    "dist_train = class_distribution(\"train\")\n",
    "dist_train.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff0b03",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Desarrollo del modelo (baseline)\n",
    "Arquitectura: **MobileNetV3Small** (preentrenada en ImageNet, *include_top=False*), GAP + Dense softmax.  \n",
    "**Regularización:** Dropout + L2.  \n",
    "**Callbacks:** EarlyStopping, ReduceLROnPlateau, ModelCheckpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7deafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_ds(root, img_size=(224,224), batch_size=16, seed=42):\n",
    "    ds_train = keras.utils.image_dataset_from_directory(\n",
    "        os.path.join(root, \"train\"),\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"categorical\",\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names_ds = ds_train.class_names\n",
    "    ds_val = keras.utils.image_dataset_from_directory(\n",
    "        os.path.join(root, \"val\"),\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"categorical\",\n",
    "        seed=seed,\n",
    "        shuffle=False\n",
    "    )\n",
    "    aug = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ])\n",
    "    ds_train = ds_train.map(lambda x,y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds_train.prefetch(AUTOTUNE), ds_val.prefetch(AUTOTUNE), class_names_ds\n",
    "\n",
    "def build_model(num_classes, input_shape=(224,224,3), dropout=0.3, l2_reg=1e-5, train_backbone=False):\n",
    "    base = keras.applications.MobileNetV3Small(include_top=False, input_shape=input_shape, weights=\"imagenet\")\n",
    "    base.trainable = train_backbone\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.applications.mobilenet_v3.preprocess_input(inputs)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e223448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3312 files belonging to 37 classes.\n",
      "Found 368 files belonging to 37 classes.\n",
      "Epoch 1/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 61ms/step - accuracy: 0.1980 - loss: 3.1659 - val_accuracy: 0.7092 - val_loss: 1.1285 - learning_rate: 0.0010\n",
      "Epoch 2/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.6471 - loss: 1.2294 - val_accuracy: 0.7663 - val_loss: 0.7875 - learning_rate: 0.0010\n",
      "Epoch 3/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - accuracy: 0.7444 - loss: 0.8665 - val_accuracy: 0.7962 - val_loss: 0.6660 - learning_rate: 0.0010\n",
      "Epoch 4/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.7815 - loss: 0.7383 - val_accuracy: 0.8152 - val_loss: 0.5965 - learning_rate: 0.0010\n",
      "Epoch 5/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.8019 - loss: 0.6639 - val_accuracy: 0.8234 - val_loss: 0.5753 - learning_rate: 0.0010\n",
      "Epoch 6/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8148 - loss: 0.5755 - val_accuracy: 0.8397 - val_loss: 0.5330 - learning_rate: 0.0010\n",
      "Epoch 7/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - accuracy: 0.8261 - loss: 0.5454 - val_accuracy: 0.8342 - val_loss: 0.5229 - learning_rate: 0.0010\n",
      "Epoch 8/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - accuracy: 0.8475 - loss: 0.4950 - val_accuracy: 0.8288 - val_loss: 0.5024 - learning_rate: 0.0010\n",
      "Epoch 9/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.8546 - loss: 0.4715 - val_accuracy: 0.8478 - val_loss: 0.4824 - learning_rate: 0.0010\n",
      "Epoch 10/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - accuracy: 0.8516 - loss: 0.4526 - val_accuracy: 0.8505 - val_loss: 0.4868 - learning_rate: 0.0010\n",
      "Epoch 11/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.8716 - loss: 0.4184 - val_accuracy: 0.8451 - val_loss: 0.4864 - learning_rate: 0.0010\n",
      "Epoch 12/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.8677 - loss: 0.4029 - val_accuracy: 0.8478 - val_loss: 0.4944 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobilenetV3small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,349</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobilenetV3small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │        \u001b[38;5;34m21,349\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,003,169</span> (3.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,003,169\u001b[0m (3.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,349</span> (83.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,349\u001b[0m (83.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,700</span> (166.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m42,700\u001b[0m (166.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento (baseline)\n",
    "BATCH = 16\n",
    "EPOCHS_BASE = 12\n",
    "LR_BASE = 1e-3\n",
    "\n",
    "ds_train, ds_val, class_names_ds = load_ds(DATA_ROOT, batch_size=BATCH)\n",
    "assert class_names_ds == class_names, \"El orden de clases debe coincidir con el de las carpetas.\"\n",
    "\n",
    "model = build_model(num_classes=len(class_names), train_backbone=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(LR_BASE),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    callbacks.CSVLogger(\"runs/train_log.csv\", append=False),\n",
    "]\n",
    "\n",
    "hist_base = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS_BASE, callbacks=cbs)\n",
    "\n",
    "# guardar nombres de clase\n",
    "with open(\"runs/class_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(class_names, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c129a4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Ajuste del modelo (fine‑tuning)\n",
    "Descongelamos un **30%** final del backbone y entrenamos con **LR bajo**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad2202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - accuracy: 0.6523 - loss: 1.1151 - val_accuracy: 0.8397 - val_loss: 0.4922 - learning_rate: 1.0000e-05\n",
      "Epoch 2/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - accuracy: 0.7068 - loss: 0.9139 - val_accuracy: 0.8315 - val_loss: 0.5125 - learning_rate: 1.0000e-05\n",
      "Epoch 3/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - accuracy: 0.7580 - loss: 0.8002 - val_accuracy: 0.8288 - val_loss: 0.5219 - learning_rate: 1.0000e-05\n",
      "Epoch 4/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 84ms/step - accuracy: 0.7450 - loss: 0.7921 - val_accuracy: 0.8261 - val_loss: 0.5182 - learning_rate: 1.0000e-05\n",
      "Epoch 5/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - accuracy: 0.7839 - loss: 0.6751 - val_accuracy: 0.8288 - val_loss: 0.5119 - learning_rate: 5.0000e-06\n",
      "Epoch 6/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 84ms/step - accuracy: 0.7711 - loss: 0.7155 - val_accuracy: 0.8261 - val_loss: 0.5066 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tuning \n",
    "UNFREEZE_RATIO = 0.30\n",
    "EPOCHS_FT = 8\n",
    "LR_FT = 1e-5\n",
    "\n",
    "model_ft = keras.models.load_model(\"runs/best.keras\")\n",
    "\n",
    "# identificar backbone (submodelo con muchas capas)\n",
    "backbone = None\n",
    "for lyr in model_ft.layers:\n",
    "    if isinstance(lyr, keras.Model) and len(lyr.layers) > 10:\n",
    "        backbone = lyr; break\n",
    "assert backbone is not None, \"No se encontró el backbone\"\n",
    "\n",
    "n = len(backbone.layers)\n",
    "cut = int(n * (1 - UNFREEZE_RATIO))\n",
    "for i, layer in enumerate(backbone.layers):\n",
    "    layer.trainable = (i >= cut)\n",
    "\n",
    "model_ft.compile(optimizer=keras.optimizers.Adam(LR_FT),\n",
    "                 loss=\"categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "cbs_ft = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best_finetune.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    callbacks.CSVLogger(\"runs/train_log_finetune.csv\", append=False),\n",
    "]\n",
    "\n",
    "hist_ft = model_ft.fit(ds_train, validation_data=ds_val, epochs=EPOCHS_FT, callbacks=cbs_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd6d70",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Resultados (test)\n",
    "Calculamos **accuracy** y **F1 macro**, y graficamos la **matriz de confusión**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e63eb960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3669 files belonging to 37 classes.\n",
      "Test accuracy = 0.8340147179067866\n",
      "Test F1 macro = 0.8341794081890155\n",
      "\n",
      "Classification report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                Abyssinian     0.9036    0.7653    0.8287        98\n",
      "                    Bengal     0.8125    0.6500    0.7222       100\n",
      "                    Birman     0.7117    0.7900    0.7488       100\n",
      "                    Bombay     0.9875    0.8977    0.9405        88\n",
      "         British_Shorthair     0.8642    0.7000    0.7735       100\n",
      "              Egyptian_Mau     0.9318    0.8454    0.8865        97\n",
      "                Maine_Coon     0.6639    0.7900    0.7215       100\n",
      "                   Persian     0.8636    0.7600    0.8085       100\n",
      "                   Ragdoll     0.5692    0.7400    0.6435       100\n",
      "              Russian_Blue     0.7177    0.8900    0.7946       100\n",
      "                   Siamese     0.9419    0.8100    0.8710       100\n",
      "                    Sphynx     0.8942    0.9300    0.9118       100\n",
      "          american_bulldog     0.6069    0.8800    0.7184       100\n",
      " american_pit_bull_terrier     0.7963    0.4300    0.5584       100\n",
      "              basset_hound     0.8571    0.8400    0.8485       100\n",
      "                    beagle     0.8144    0.7900    0.8020       100\n",
      "                     boxer     0.8636    0.7677    0.8128        99\n",
      "                 chihuahua     0.7416    0.6600    0.6984       100\n",
      "    english_cocker_spaniel     0.8936    0.8400    0.8660       100\n",
      "            english_setter     0.8333    0.8500    0.8416       100\n",
      "        german_shorthaired     0.9588    0.9300    0.9442       100\n",
      "            great_pyrenees     0.8962    0.9500    0.9223       100\n",
      "                  havanese     0.8142    0.9200    0.8638       100\n",
      "             japanese_chin     0.9010    0.9100    0.9055       100\n",
      "                  keeshond     0.9500    0.9596    0.9548        99\n",
      "                leonberger     0.9500    0.9500    0.9500       100\n",
      "        miniature_pinscher     0.9565    0.6600    0.7811       100\n",
      "              newfoundland     0.9327    0.9700    0.9510       100\n",
      "                pomeranian     0.8889    0.9600    0.9231       100\n",
      "                       pug     1.0000    0.8300    0.9071       100\n",
      "             saint_bernard     0.7917    0.9500    0.8636       100\n",
      "                   samoyed     0.9286    0.9100    0.9192       100\n",
      "          scottish_terrier     0.9143    0.9697    0.9412        99\n",
      "                 shiba_inu     0.8364    0.9200    0.8762       100\n",
      "staffordshire_bull_terrier     0.4711    0.6404    0.5429        89\n",
      "           wheaten_terrier     0.9158    0.8700    0.8923       100\n",
      "         yorkshire_terrier     0.9388    0.9200    0.9293       100\n",
      "\n",
      "                  accuracy                         0.8340      3669\n",
      "                 macro avg     0.8463    0.8337    0.8342      3669\n",
      "              weighted avg     0.8468    0.8340    0.8346      3669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carga test y evaluación\n",
    "def load_all(split_dir):\n",
    "    ds = keras.utils.image_dataset_from_directory(split_dir, image_size=IMG_SIZE, batch_size=32, label_mode=\"categorical\", shuffle=False)\n",
    "    Xs, Ys = [], []\n",
    "    for x,y in ds:\n",
    "        Xs.append(x.numpy()); Ys.append(y.numpy())\n",
    "    return np.vstack(Xs), np.vstack(Ys), ds.class_names\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "X_test, Y_test, class_names_test = load_all(os.path.join(DATA_ROOT, \"test\"))\n",
    "m_final_path = \"runs/best_finetune.keras\" if os.path.exists(\"runs/best_finetune.keras\") else \"runs/best.keras\"\n",
    "m_final = keras.models.load_model(m_final_path)\n",
    "\n",
    "P = m_final.predict(X_test, verbose=0)\n",
    "y_true = Y_test.argmax(1); y_pred = P.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(\"Test accuracy =\", acc)\n",
    "print(\"Test F1 macro =\", f1m)\n",
    "\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot CM\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools, os\n",
    "os.makedirs(\"reports/figures\", exist_ok=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(len(class_names))\n",
    "plt.xticks(ticks, class_names, rotation=90)\n",
    "plt.yticks(ticks, class_names)\n",
    "thresh = cm.max()/2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=7)\n",
    "plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
    "plt.savefig(\"reports/figures/confusion_matrix.png\", dpi=160)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cac82",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Aplicación (predicción en imágenes nuevas)\n",
    "Función que recibe una **ruta a imagen** y devuelve **Top‑5** predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beagle', 0.5181585550308228),\n",
       " ('basset_hound', 0.23956629633903503),\n",
       " ('shiba_inu', 0.07332886755466461),\n",
       " ('chihuahua', 0.05625345930457115),\n",
       " ('american_bulldog', 0.03815215826034546)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "CLASS_JSON = \"runs/class_names.json\"\n",
    "with open(CLASS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    CLASS_NAMES = json.load(f)\n",
    "\n",
    "def predict_image(path, model_path=None, topk=5):\n",
    "    model_path = model_path or (\"runs/best_finetune.keras\" if os.path.exists(\"runs/best_finetune.keras\") else \"runs/best.keras\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    img = Image.open(path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    x = np.array(img, dtype=\"float32\")[None, ...]\n",
    "    x = keras.applications.mobilenet_v3.preprocess_input(x)\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    idxs = probs.argsort()[-topk:][::-1]\n",
    "    return [(CLASS_NAMES[i], float(probs[i])) for i in idxs]\n",
    "\n",
    "# Ejemplo:\n",
    "preds = predict_image(\"C:/Users/Luis/Documents/7_semestre/data_reto_bloque2/Implementacion_modelo_DeepLearning/runsbeagle2.png\")\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75b387",
   "metadata": {},
   "source": [
    "## Conclusiones \n",
    "\n",
    "* El modelo generaliza bien en promedio, pero muestra confusiones entre razas pequeñas/parecidas (p. ej., miniature_pinscher / chihuahua / beagle), sobre todo con ángulos raros y variaciones de iluminación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b5815",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retoHousing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

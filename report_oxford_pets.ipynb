{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537440ee",
   "metadata": {},
   "source": [
    "\n",
    "# Clasificación de Razas de Mascotas — Oxford‑IIIT Pet (Keras/TensorFlow)\n",
    "\n",
    "**Luis Ubaldo Balderas Sanchez A01751150** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4af8a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introducción\n",
    "\n",
    "**Problema:** Clasificar imágenes de perros y gatos en **37 razas** usando **deep learning**.  \n",
    "**Relevancia:** Aplicaciones en catalogación automática, búsquedas por imagen y apoyo a refugios/ONGs para identificación.\n",
    "\n",
    "**Objetivo:** Entrenar un modelo CNN (transfer learning) y **mejorarlo** mediante fine‑tuning/regularización, evaluando con métricas de validación y prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446acd9c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Datos\n",
    "\n",
    "**Fuente:** Oxford‑IIIT Pet (via `tensorflow_datasets`).  \n",
    "- 37 clases de perros y gatos.  \n",
    "- En este notebook descargamos los datos automáticamente \n",
    "\n",
    "**Split:** Usaremos `train/val/test` (10% de train pasa a validación).  \n",
    "**Preprocesamiento:** Redimensionado a 224×224, normalización según la *preprocess* de la arquitectura base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b13754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, json, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from keras import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51215986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1 3.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, json, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from keras import layers, callbacks\n",
    "\n",
    "# Config reproducibilidad (opcional)\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = \"data/processed/pets\"\n",
    "IMG_SIZE = (224, 224)\n",
    "VAL_FRACTION = 0.10\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "print(tf.__version__, keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1ad2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Descarga/lectura del dataset con TFDS y exportación a carpetas\n",
    "def _save_example(img, label, class_names, split_dir, idx):\n",
    "    cls = class_names[int(label)]\n",
    "    class_dir = os.path.join(split_dir, cls)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(tf.clip_by_value(img, 0, 255), tf.uint8)\n",
    "    tf.io.write_file(os.path.join(class_dir, f\"{cls}_{idx:06d}.jpg\"), tf.io.encode_jpeg(img))\n",
    "\n",
    "def prepare_oxford_pets():\n",
    "    ds_train = tfds.load(\"oxford_iiit_pet\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
    "    ds_test  = tfds.load(\"oxford_iiit_pet\", split=\"test\",  as_supervised=True, shuffle_files=False)\n",
    "    info = tfds.builder(\"oxford_iiit_pet\").info\n",
    "    class_names = info.features[\"label\"].names\n",
    "    \n",
    "    # materializamos para split de validación reproducible\n",
    "    train_list = list(tfds.as_numpy(ds_train))\n",
    "    n_val = math.ceil(len(train_list) * VAL_FRACTION)\n",
    "    val_list = train_list[:n_val]\n",
    "    trn_list = train_list[n_val:]\n",
    "    \n",
    "    # crear carpetas\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for c in class_names:\n",
    "            os.makedirs(os.path.join(DATA_ROOT, split, c), exist_ok=True)\n",
    "    \n",
    "    i = 0\n",
    "    for img, label in trn_list:\n",
    "        _save_example(img, label, class_names, os.path.join(DATA_ROOT, \"train\"), i); i += 1\n",
    "    j = 0\n",
    "    for img, label in val_list:\n",
    "        _save_example(img, label, class_names, os.path.join(DATA_ROOT, \"val\"), j); j += 1\n",
    "    k = 0\n",
    "    for img, label in tfds.as_numpy(ds_test):\n",
    "        _save_example(img, label, class_names, os.path.join(DATA_ROOT, \"test\"), k); k += 1\n",
    "    \n",
    "    return class_names\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, \"train\")):\n",
    "    class_names = prepare_oxford_pets()\n",
    "else:\n",
    "    # leer nombres de clase desde carpetas\n",
    "    class_names = sorted([d for d in os.listdir(os.path.join(DATA_ROOT, \"train\")) if os.path.isdir(os.path.join(DATA_ROOT, \"train\", d))])\n",
    "\n",
    "len(class_names), class_names[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f429ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birman             95\n",
       "wheaten_terrier    95\n",
       "chihuahua          95\n",
       "samoyed            94\n",
       "keeshond           94\n",
       "leonberger         94\n",
       "Persian            94\n",
       "Sphynx             94\n",
       "pomeranian         93\n",
       "great_pyrenees     93\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Visualización rápida: distribución de clases (train)\n",
    "def class_distribution(split=\"train\"):\n",
    "    root = os.path.join(DATA_ROOT, split)\n",
    "    counts = {}\n",
    "    for c in class_names:\n",
    "        cdir = os.path.join(root, c)\n",
    "        counts[c] = len([f for f in os.listdir(cdir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
    "    return pd.Series(counts).sort_values(ascending=False)\n",
    "\n",
    "dist_train = class_distribution(\"train\")\n",
    "dist_train.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff0b03",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Desarrollo del modelo (baseline)\n",
    "Arquitectura: **MobileNetV3Small** (preentrenada en ImageNet, *include_top=False*), GAP + Dense softmax.  \n",
    "**Regularización:** Dropout + L2.  \n",
    "**Callbacks:** EarlyStopping, ReduceLROnPlateau, ModelCheckpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7deafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_ds(root, img_size=(224,224), batch_size=16, seed=42):\n",
    "    ds_train = keras.utils.image_dataset_from_directory(\n",
    "        os.path.join(root, \"train\"),\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"categorical\",\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names_ds = ds_train.class_names\n",
    "\n",
    "    ds_val = keras.utils.image_dataset_from_directory(\n",
    "        os.path.join(root, \"val\"),\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"categorical\",\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Data augmentation más agresivo\n",
    "    aug = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.12),\n",
    "        layers.RandomZoom(0.15),\n",
    "        layers.RandomTranslation(0.05, 0.05),\n",
    "        layers.RandomContrast(0.2),\n",
    "    ])\n",
    "\n",
    "    ds_train = ds_train.map(\n",
    "        lambda x, y: (aug(x, training=True), y),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "    return ds_train.prefetch(AUTOTUNE), ds_val.prefetch(AUTOTUNE), class_names_ds\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    num_classes,\n",
    "    input_shape=(224, 224, 3),\n",
    "    dropout=0.3,\n",
    "    l2_reg=1e-5,\n",
    "    train_backbone=False,\n",
    "    backbone=\"mobilenet_v3_small\",\n",
    "):\n",
    "    if backbone == \"mobilenet_v3_small\":\n",
    "        Base = keras.applications.MobileNetV3Small\n",
    "        preprocess = keras.applications.mobilenet_v3.preprocess_input\n",
    "    elif backbone == \"efficientnet_b0\":\n",
    "        Base = keras.applications.EfficientNetB0\n",
    "        preprocess = keras.applications.efficientnet.preprocess_input\n",
    "    else:\n",
    "        raise ValueError(f\"Backbone no soportado: {backbone}\")\n",
    "\n",
    "    base = Base(include_top=False, input_shape=input_shape, weights=\"imagenet\")\n",
    "    base.trainable = train_backbone\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = preprocess(inputs)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(\n",
    "        num_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_reg),\n",
    "    )(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e223448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3312 files belonging to 37 classes.\n",
      "Found 368 files belonging to 37 classes.\n",
      "Epoch 1/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 125ms/step - accuracy: 0.1702 - loss: 3.3017 - val_accuracy: 0.6848 - val_loss: 1.2651 - learning_rate: 0.0010\n",
      "Epoch 2/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - accuracy: 0.5786 - loss: 1.4871 - val_accuracy: 0.7636 - val_loss: 0.8614 - learning_rate: 0.0010\n",
      "Epoch 3/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - accuracy: 0.6820 - loss: 1.1045 - val_accuracy: 0.7935 - val_loss: 0.7333 - learning_rate: 0.0010\n",
      "Epoch 4/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - accuracy: 0.7102 - loss: 0.9635 - val_accuracy: 0.8179 - val_loss: 0.6517 - learning_rate: 0.0010\n",
      "Epoch 5/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 107ms/step - accuracy: 0.7276 - loss: 0.8800 - val_accuracy: 0.8234 - val_loss: 0.6132 - learning_rate: 0.0010\n",
      "Epoch 6/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 110ms/step - accuracy: 0.7510 - loss: 0.8188 - val_accuracy: 0.8234 - val_loss: 0.5930 - learning_rate: 0.0010\n",
      "Epoch 7/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 122ms/step - accuracy: 0.7633 - loss: 0.7418 - val_accuracy: 0.8288 - val_loss: 0.5927 - learning_rate: 0.0010\n",
      "Epoch 8/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 113ms/step - accuracy: 0.7688 - loss: 0.7105 - val_accuracy: 0.8424 - val_loss: 0.5395 - learning_rate: 0.0010\n",
      "Epoch 9/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 106ms/step - accuracy: 0.7841 - loss: 0.7070 - val_accuracy: 0.8261 - val_loss: 0.5697 - learning_rate: 0.0010\n",
      "Epoch 10/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 110ms/step - accuracy: 0.7827 - loss: 0.6619 - val_accuracy: 0.8370 - val_loss: 0.5532 - learning_rate: 0.0010\n",
      "Epoch 11/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 114ms/step - accuracy: 0.8011 - loss: 0.6343 - val_accuracy: 0.8370 - val_loss: 0.5343 - learning_rate: 0.0010\n",
      "Epoch 12/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 108ms/step - accuracy: 0.8014 - loss: 0.6362 - val_accuracy: 0.8397 - val_loss: 0.5464 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobilenetV3small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,349</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobilenetV3small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │        \u001b[38;5;34m21,349\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,003,169</span> (3.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,003,169\u001b[0m (3.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,349</span> (83.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,349\u001b[0m (83.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,700</span> (166.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m42,700\u001b[0m (166.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento (baseline)\n",
    "BATCH = 16\n",
    "EPOCHS_BASE = 12\n",
    "LR_BASE = 1e-3\n",
    "\n",
    "ds_train, ds_val, class_names_ds = load_ds(DATA_ROOT, batch_size=BATCH)\n",
    "assert class_names_ds == class_names, \"El orden de clases debe coincidir con el de las carpetas.\"\n",
    "\n",
    "model = build_model(num_classes=len(class_names), train_backbone=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(LR_BASE),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    callbacks.CSVLogger(\"runs/train_log.csv\", append=False),\n",
    "]\n",
    "\n",
    "hist_base = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS_BASE, callbacks=cbs)\n",
    "\n",
    "# guardar nombres de clase\n",
    "with open(\"runs/class_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(class_names, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf62117",
   "metadata": {},
   "source": [
    "## EfficentNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ba0f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3312 files belonging to 37 classes.\n",
      "Found 368 files belonging to 37 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 319ms/step - accuracy: 0.3742 - loss: 2.5049 - val_accuracy: 0.8152 - val_loss: 0.7122 - learning_rate: 0.0010\n",
      "Epoch 2/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 363ms/step - accuracy: 0.7742 - loss: 0.8827 - val_accuracy: 0.8668 - val_loss: 0.4989 - learning_rate: 0.0010\n",
      "Epoch 3/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 265ms/step - accuracy: 0.8210 - loss: 0.6624 - val_accuracy: 0.8859 - val_loss: 0.4148 - learning_rate: 0.0010\n",
      "Epoch 4/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 260ms/step - accuracy: 0.8542 - loss: 0.5417 - val_accuracy: 0.8804 - val_loss: 0.3832 - learning_rate: 0.0010\n",
      "Epoch 5/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 241ms/step - accuracy: 0.8579 - loss: 0.4809 - val_accuracy: 0.8859 - val_loss: 0.3663 - learning_rate: 0.0010\n",
      "Epoch 6/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 288ms/step - accuracy: 0.8780 - loss: 0.4237 - val_accuracy: 0.8913 - val_loss: 0.3496 - learning_rate: 0.0010\n",
      "Epoch 7/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 222ms/step - accuracy: 0.8845 - loss: 0.3815 - val_accuracy: 0.8913 - val_loss: 0.3259 - learning_rate: 0.0010\n",
      "Epoch 8/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 231ms/step - accuracy: 0.9005 - loss: 0.3565 - val_accuracy: 0.9022 - val_loss: 0.3299 - learning_rate: 0.0010\n",
      "Epoch 9/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 225ms/step - accuracy: 0.8947 - loss: 0.3467 - val_accuracy: 0.8859 - val_loss: 0.3370 - learning_rate: 0.0010\n",
      "Epoch 10/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 297ms/step - accuracy: 0.9105 - loss: 0.3139 - val_accuracy: 0.9022 - val_loss: 0.3196 - learning_rate: 0.0010\n",
      "Epoch 11/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 281ms/step - accuracy: 0.9132 - loss: 0.3092 - val_accuracy: 0.8967 - val_loss: 0.3195 - learning_rate: 0.0010\n",
      "Epoch 12/12\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 215ms/step - accuracy: 0.9188 - loss: 0.2749 - val_accuracy: 0.8995 - val_loss: 0.3261 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">47,397</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │        \u001b[38;5;34m47,397\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,191,764</span> (15.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,191,764\u001b[0m (15.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,397</span> (185.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,397\u001b[0m (185.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,796</span> (370.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m94,796\u001b[0m (370.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entrenamiento con EfficientNet-B0 (Experimento arquitectura)\n",
    "BATCH = 16\n",
    "EPOCHS_BASE = 12\n",
    "LR_BASE = 1e-3\n",
    "\n",
    "ds_train_eff, ds_val_eff, class_names_eff = load_ds(DATA_ROOT, batch_size=BATCH)\n",
    "assert class_names_eff == class_names, \"El orden de clases debe coincidir\"\n",
    "\n",
    "model_eff = build_model(\n",
    "    num_classes=len(class_names),\n",
    "    train_backbone=False,\n",
    "    backbone=\"efficientnet_b0\"\n",
    ")\n",
    "\n",
    "model_eff.compile(\n",
    "    optimizer=keras.optimizers.Adam(LR_BASE),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "cbs_eff = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best_effnet.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "]\n",
    "\n",
    "hist_eff = model_eff.fit(ds_train_eff, validation_data=ds_val_eff, epochs=EPOCHS_BASE, callbacks=cbs_eff)\n",
    "model_eff.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c129a4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Ajuste del modelo (fine‑tuning)\n",
    "Descongelamos un **30%** final del backbone y entrenamos con **LR bajo**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad2202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.6245 - loss: 1.2144 - val_accuracy: 0.8397 - val_loss: 0.5489 - learning_rate: 1.0000e-05\n",
      "Epoch 2/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 95ms/step - accuracy: 0.6773 - loss: 1.0760 - val_accuracy: 0.8370 - val_loss: 0.5696 - learning_rate: 1.0000e-05\n",
      "Epoch 3/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 100ms/step - accuracy: 0.6904 - loss: 1.0047 - val_accuracy: 0.8315 - val_loss: 0.5730 - learning_rate: 1.0000e-05\n",
      "Epoch 4/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 100ms/step - accuracy: 0.7137 - loss: 0.9460 - val_accuracy: 0.8234 - val_loss: 0.5741 - learning_rate: 1.0000e-05\n",
      "Epoch 5/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 108ms/step - accuracy: 0.7251 - loss: 0.8724 - val_accuracy: 0.8261 - val_loss: 0.5705 - learning_rate: 5.0000e-06\n",
      "Epoch 6/8\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 100ms/step - accuracy: 0.7341 - loss: 0.8517 - val_accuracy: 0.8234 - val_loss: 0.5666 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tuning \n",
    "UNFREEZE_RATIO = 0.30\n",
    "EPOCHS_FT = 8\n",
    "LR_FT = 1e-5\n",
    "\n",
    "model_ft = keras.models.load_model(\"runs/best.keras\")\n",
    "\n",
    "# identificar backbone (submodelo con muchas capas)\n",
    "backbone = None\n",
    "for lyr in model_ft.layers:\n",
    "    if isinstance(lyr, keras.Model) and len(lyr.layers) > 10:\n",
    "        backbone = lyr; break\n",
    "assert backbone is not None, \"No se encontró el backbone\"\n",
    "\n",
    "n = len(backbone.layers)\n",
    "cut = int(n * (1 - UNFREEZE_RATIO))\n",
    "for i, layer in enumerate(backbone.layers):\n",
    "    layer.trainable = (i >= cut)\n",
    "\n",
    "model_ft.compile(optimizer=keras.optimizers.Adam(LR_FT),\n",
    "                 loss=\"categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "cbs_ft = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best_finetune.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    callbacks.CSVLogger(\"runs/train_log_finetune.csv\", append=False),\n",
    "]\n",
    "\n",
    "hist_ft = model_ft.fit(ds_train, validation_data=ds_val, epochs=EPOCHS_FT, callbacks=cbs_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd6d70",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Resultados (test)\n",
    "Calculamos **accuracy** y **F1 macro**, y graficamos la **matriz de confusión**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63eb960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3669 files belonging to 37 classes.\n",
      "Test accuracy = 0.8233851185609158\n",
      "Test F1 macro = 0.8227004193717424\n",
      "\n",
      "Classification report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                Abyssinian     0.9000    0.7347    0.8090        98\n",
      "                    Bengal     0.8533    0.6400    0.7314       100\n",
      "                    Birman     0.6825    0.8600    0.7611       100\n",
      "                    Bombay     0.9750    0.8864    0.9286        88\n",
      "         British_Shorthair     0.7849    0.7300    0.7565       100\n",
      "              Egyptian_Mau     0.9419    0.8351    0.8852        97\n",
      "                Maine_Coon     0.6471    0.7700    0.7032       100\n",
      "                   Persian     0.8315    0.7400    0.7831       100\n",
      "                   Ragdoll     0.6095    0.6400    0.6244       100\n",
      "              Russian_Blue     0.7596    0.7900    0.7745       100\n",
      "                   Siamese     0.8936    0.8400    0.8660       100\n",
      "                    Sphynx     0.9247    0.8600    0.8912       100\n",
      "          american_bulldog     0.6800    0.8500    0.7556       100\n",
      " american_pit_bull_terrier     0.7586    0.4400    0.5570       100\n",
      "              basset_hound     0.8810    0.7400    0.8043       100\n",
      "                    beagle     0.7778    0.7700    0.7739       100\n",
      "                     boxer     0.7959    0.7879    0.7919        99\n",
      "                 chihuahua     0.6887    0.7300    0.7087       100\n",
      "    english_cocker_spaniel     0.8652    0.7700    0.8148       100\n",
      "            english_setter     0.7632    0.8700    0.8131       100\n",
      "        german_shorthaired     0.9320    0.9600    0.9458       100\n",
      "            great_pyrenees     0.8750    0.9100    0.8922       100\n",
      "                  havanese     0.8034    0.9400    0.8664       100\n",
      "             japanese_chin     0.9348    0.8600    0.8958       100\n",
      "                  keeshond     0.9320    0.9697    0.9505        99\n",
      "                leonberger     0.9894    0.9300    0.9588       100\n",
      "        miniature_pinscher     0.9306    0.6700    0.7791       100\n",
      "              newfoundland     0.9065    0.9700    0.9372       100\n",
      "                pomeranian     0.8230    0.9300    0.8732       100\n",
      "                       pug     1.0000    0.8600    0.9247       100\n",
      "             saint_bernard     0.7422    0.9500    0.8333       100\n",
      "                   samoyed     0.8846    0.9200    0.9020       100\n",
      "          scottish_terrier     0.9057    0.9697    0.9366        99\n",
      "                 shiba_inu     0.8261    0.9500    0.8837       100\n",
      "staffordshire_bull_terrier     0.4425    0.5618    0.4950        89\n",
      "           wheaten_terrier     0.9355    0.8700    0.9016       100\n",
      "         yorkshire_terrier     0.9216    0.9400    0.9307       100\n",
      "\n",
      "                  accuracy                         0.8234      3669\n",
      "                 macro avg     0.8324    0.8228    0.8227      3669\n",
      "              weighted avg     0.8329    0.8234    0.8232      3669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carga test y evaluación\n",
    "def load_all(split_dir):\n",
    "    ds = keras.utils.image_dataset_from_directory(split_dir, image_size=IMG_SIZE, batch_size=32, label_mode=\"categorical\", shuffle=False)\n",
    "    Xs, Ys = [], []\n",
    "    for x,y in ds:\n",
    "        Xs.append(x.numpy()); Ys.append(y.numpy())\n",
    "    return np.vstack(Xs), np.vstack(Ys), ds.class_names\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "X_test, Y_test, class_names_test = load_all(os.path.join(DATA_ROOT, \"test\"))\n",
    "m_final_path = \"runs/best_finetune.keras\" if os.path.exists(\"runs/best_finetune.keras\") else \"runs/best.keras\"\n",
    "m_final = keras.models.load_model(m_final_path)\n",
    "\n",
    "P = m_final.predict(X_test, verbose=0)\n",
    "y_true = Y_test.argmax(1); y_pred = P.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(\"Test accuracy =\", acc)\n",
    "print(\"Test F1 macro =\", f1m)\n",
    "\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot CM\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools, os\n",
    "os.makedirs(\"reports/figures\", exist_ok=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(len(class_names))\n",
    "plt.xticks(ticks, class_names, rotation=90)\n",
    "plt.yticks(ticks, class_names)\n",
    "thresh = cm.max()/2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=7)\n",
    "plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
    "plt.savefig(\"reports/figures/confusion_matrix.png\", dpi=160)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cac82",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Aplicación (predicción en imágenes nuevas)\n",
    "Función que recibe una **ruta a imagen** y devuelve **Top‑5** predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a4db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('basset_hound', 0.498619019985199),\n",
       " ('beagle', 0.25160694122314453),\n",
       " ('shiba_inu', 0.10371589660644531),\n",
       " ('staffordshire_bull_terrier', 0.06120742857456207),\n",
       " ('chihuahua', 0.058003440499305725)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "CLASS_JSON = \"runs/class_names.json\"\n",
    "with open(CLASS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    CLASS_NAMES = json.load(f)\n",
    "\n",
    "def predict_image(path, model_path=None, topk=5):\n",
    "    model_path = model_path or (\"runs/best_finetune.keras\" if os.path.exists(\"runs/best_finetune.keras\") else \"runs/best.keras\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    img = Image.open(path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    x = np.array(img, dtype=\"float32\")[None, ...]\n",
    "    x = keras.applications.mobilenet_v3.preprocess_input(x)\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    idxs = probs.argsort()[-topk:][::-1]\n",
    "    return [(CLASS_NAMES[i], float(probs[i])) for i in idxs]\n",
    "\n",
    "# Ejemplo:\n",
    "preds = predict_image(\"C:/Users/Luis/Documents/7_semestre/data_reto_bloque2/Implementacion_modelo_DeepLearning/runs/beagle2.png\")\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75b387",
   "metadata": {},
   "source": [
    "## Conclusiones \n",
    "\n",
    "* El modelo generaliza bien en promedio, pero muestra confusiones entre razas pequeñas/parecidas (p. ej., miniature_pinscher / chihuahua / beagle), sobre todo con ángulos raros y variaciones de iluminación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b5815",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retoHousing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
